%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Signal region optimisation *********
%*******************************************************************************


\chapter{Signal region optimisation}

\ifpdf
    \graphicspath{{chapter-optimisation/Figs/Raster/}{chapter-optimisation/Figs/PDF/}{chapter-optimisation/Figs/}}
\else
    \graphicspath{{chapter-optimisation/Figs/Vector/}{chapter-optimisation/Figs/}}
\fi

\glsreset{sr}

In order to discover the rare signals predicted by the \gls{susy} models considered, dedicated kinematic regions enriched in signal events, so called \gls{sr} are constructed. They are optimised to be able to discover a maximum number of the signal models considered in the analysis. In this chapter, the \gls{sr} optimisation procedures leading to the final \glspl{sr} are introduced and discussed. 

\section{Optimisation methods}

All optimisation methods used in the following need a figure of merit that should be maximised in order to define the best performing setup. While the multidimensional cut scan in \cref{sec:n-dim-scan} and the N-1 plots approach in \cref{sec:n-1-scan} use the binomial discovery significance $Z_\mathrm{B}$ introduced in~\cref{sec:sensitivity_estimation}, the fit scan procedure in \cref{sec:fit-scan} aims to maximise the area of the expected exclusion contour.

\subsection{Multidimensional cut scan}\label{sec:n-dim-scan}

The first optimisation method used for designing the \glspl{sr} is an $N$-dimensional cut scan using $N$ observables. For each unique combination of requirements on the set of considered observables, the expected signal and background rate as well as the statistical uncertainty on the background rate is determined from the \gls{mc} samples. As this takes a non-negligible amount of time, it is crucial to restrict the amount of cut combinations. By comparing with distributions at preselection level as \eg shown in \cref{fig:norm_obs}, a set of discrete cuts can be defined for each observable. In practice, a total number of $\mathcal{O}(10^7-10^8)$ cut combinations can still be tested on a single machine with a reasonable turnaround time. 

After determining the excepted event rates and statistical uncertainties, the different cut combination are binned into a predefined number of signal efficiency bins. For each bin, the background rejection is subsequently maximised, \ie the cut combination with the highest background is chosen as a candidate combination for the respective signal efficiency bin. Cut combination candidates maximising the background rejection are assumed to also maximise the discovery significance. With the significance definition used herein, this is in general a valid assumption, and the significance tends to increases with decreasing background rate, even when the statistical uncertainty on the background estimation increases due to tighter requirements and less available \gls{mc} statistics. This procedure effectively generates a \gls{roc} curve. As only a small subset of all tested cut combinations are selected as candidates and lie on the \gls{roc} curve, more computationally intensive calculations can be performed, as \eg calculating the discovery significance.

A common problem of $N$-dimensional scans is the concept of \textit{overtightening} the selections given the available \gls{mc} statistics. Since the cross sections of the considered \gls{susy} process are many orders of magnitude smaller than those of most of the \gls{sm} processes, it is necessary to apply tight requirements on the kinematic observables in order to achieve a significant signal-to-background separation. However, due to the finite amount of \gls{mc} statistics available, many of the more extreme cut combinations select kinematic regions where not enough \gls{mc} statistics are available for a reasonable estimation of the background rates. Thus, by maximising the background rejection, it may happen that cut combinations are selected where the mere lack of background \gls{mc} statistics causes a high significance value. As the significance values obtained for such configurations are naturally not trustworthy, they need to be avoided. 

In the $N$-dimensional cut scan implementation used herein, the available \gls{mc} datasets are split in two statistically independent, equally sized subsets. This allows to compute two independent values for the discovery significance for each cut combination candidate, as well as having two \gls{roc} curves for each scan. A large difference in either the significance values or the \gls{roc} curves then is a clear indication that too tight cuts are applied for the available \gls{mc} statistics. In addition, requirements on the minimum number of raw \gls{mc} events for different background processes, as well as the maximum allowed statistical uncertainty on a given process, are applied. In the following, the $N$-dimensional cut scan implementation provided by \texttt{ahoi}~\cite{ahoi} is used.


\subsection{N-1 plots}\label{sec:n-1-scan}

Instead of performing a brute-force scan of a large set of cut combinations, a more manual approach, using repeated one-dimensional scans can be employed. In so-called N-1 plots, the variable distributions of the background components as well as exemplary signal processes are plotted together with the significance achieved when applying a cut on each value on the \textit{x}-axis of the plotted distribution. This allows to investigate the impact that a cut on a single observable has on the overall significance value. By repeating this process for each variable considered, it is possible to iteratively approach a cut combination yielding comparable results to a brute-force cut scan. Especially when considering a large number of variables, this manual approach quickly becomes very cumbersome and runs into the risk of missing optimal cut combinations an $N$-dimensional cut scan would have found.

For this reason, N-1 plots are used in the following to verify and fine-tune the results from $N$-dimensional cut scans.
 
\subsection{Fit scans}\label{sec:fit-scan}

The last of applied optimisation methods uses simplified fit setups in order to compute the expected exclusion limits for various signal region candidates obtained using the previous methods. The simplified fit setups estimate the background contribution purely from \gls{mc} and only include an estimation of the systematic uncertainties on the background estimate of 30\%, correlated over all signal region bins\unsure{really correlated? check again}. Statistical uncertainties on the background estimation from the limited \gls{mc} statistics are also included. Similar to the previous methods, many different configurations can be tested, aiming to maximise the size of the expected exclusion contour.

Although being a very simple fit configuration, the statistical inference can take a significant amount of computation time. In order to keep the number of configurations to be tested at a manageable level, the signal region candidates obtained from the previous methods are only varied to a limited degree, assuming that they were already close to optimal in terms of expected exclusion area.

\section{Optimisation procedure}

The optimisation of the \glspl{sr} uses experience from past analyses investigating the same signal model in the same final state~\cite{SUSY-2013-23,SUSY-2017-01}, all the while exploring new observables and \gls{sr} configurations optimised for the full Run~2 dataset. 



