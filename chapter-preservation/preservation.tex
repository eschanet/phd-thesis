%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Analysis Preservation *********
%*******************************************************************************

\chapter{Analysis preservation}\label{ch:preservation}

\ifpdf
    \graphicspath{{chapter-preservation/Figs/Raster/}{chapter-preservation/Figs/PDF/}{chapter-preservation/Figs/}}
\else
    \graphicspath{{chapter-preservation/Figs/Vector/}{chapter-preservation/Figs/}}
\fi

Today's particle physics experiments operate are designed to collect physics data over a span over several decades. They thus operate at scales that makes it impossible for the experiments to be repeated in the foreseeable future. The data taken at these experiments and physics results derived are thus extremely valuable and major problems arise from a scientific reproducibility point of view. In this chapter, the reproducibility problems directly connected to an individual analysis are discussed, and approaches taken in view of analysis preservation are presented.

\section{The case for reinterpretations}\label{sec:reinterpretations}

\subsection{Motivation}
Designing and executing searches for \gls{bsm} physics requires a large amount of human and computational resources. As laid out in the previous part of this work, an analysis generally aims to define a phase space region where a given signal model can be efficiently discriminated against \gls{sm} background. Although the careful design of such regions already requires significant amount of resources, it constitutes only a fraction of the work necessary for concluding the search. Contributions from \gls{sm} processes need to be estimated, usually requiring expensive \gls{mc} simulation and the development of background estimation strategies. Systematic uncertainties arising from numerous sources need to be considered and estimated. Furthermore, simulated signal events also need to be generated, reconstructed and processed through the event selection. Recorded data also needs to be reconstructed and processed through the event selection. Only after all three processing pipelines are concluded can the likelihood be built and statistical inference can be performed, produced the results like \eg limits on model parameters can be obtained. \Cref{fig:pipeline_analysis} illustrates the main data pipelines in an analysis, including their most important processing steps.

Due to the substantial amount of resources necessary for each analysis, it is not feasible to develop dedicated searches optimised for every possible signal model. Instead, analyses are typically interpreted in a finite set of \gls{bsm} models. Still, it is very likely that any given analysis is sensitive to a variety of different \gls{bsm} models not considered in the original publication. There is a real possibility that \gls{susy} is accessible at the energies of the \gls{lhc} but is still hiding in unexpected places or the complex topologies arising from complete \gls{susy} models. 

Consequently, it is not surprising that there is significant interest in the high energy physics community in reinterpreting \gls{bsm} searches in different signal models. Reinterpretations of published \gls{bsm} searches routinely happen both within as well as outside of the ATLAS collaboration\improvement{some examples}. For theorists, the analyses performed by the collaboration represent the only available windows into the dataset recorded. Reinterpretations of reproducible analyses are thus the only possibility to determine the implications of \gls{lhc} data for a variety of models~\cite{reinterpretation_workshop}. Likewise, within the experimental collaborations, reinterpretations can additionally serve as powerful guides for designing the search program. Reinterpretations of ATLAS \gls{susy} searches in more complete \gls{susy} models like the \gls{pmssm}(as was done after Run~1 of the \gls{lhc}, see \reference\cite{pMSSM-scan-run1:2015baa}) not only allow to state a combined sensitivity of ATLAS to more realistic \gls{susy} models, but also enables the collaboration to identify potential blind spots and parameter regions still uncovered by existing analyses. Reinterpretations of existing analyses are thus highly desirable and vital for designing future searches with a maximal scientific relevance.  

 \begin{figure}
	\centering\includegraphics[width=\textwidth]{pipeline}
	\caption{Full analysis workflow including the three main processing pipelines for deriving background and signal estimates as well as observed data rates. The outputs of the three processing pipelines are combined into a likelihood forming the basis for statistical inference. In a \textsc{Recast} setup, the background and data paths are archived (\eg by preserving the partial likelihood created from the background estimates and the observed data), and the signal path is fully preserved such that it can be re-run at any time. Figure recreated from \reference\cite{ATL-PHYS-PUB-2019-032}.}
	\label{fig:pipeline_analysis}
\end{figure}

\subsection{Necessary ingredients}

As the event selection of an analysis is fixed, the background estimates and observed data in the targeted regions of interest do not change and can be archived in a suitable format. Reinterpreting a search in the light of a new signal model consequently only requires the signal pipeline in~\cref{fig:pipeline_analysis} to be run again, in order to derive the signal estimates that serve as input for the statistical inference. As the data and background processing pipelines shown in~\cref{fig:pipeline_analysis} only enter the statistical inference as estimated event rates, the volume of data that needs to be archived is significantly smaller than the original input data. As will be discussed in~\cref{sec:full_likelihood}, it has recently become technically possible to directly preserve the partial analysis likelihood built from the background estimates and observed data and including all details of the statistical model used for inference. Once the signal estimates are known, a new full analysis likelihood can be built, and the viability of the new signal model can be tested. 

Different approaches exist for deriving signal estimates. Manifestly the most precise approach involves running the original analysis using a different \gls{bsm} model. As this requires to preserve the entirety of the original software and workflows used in the analysis, this is arguably the most involved approach. A framework designed to facilitate such an effort, called \textsc{Recast}, has originally been proposed in \reference\cite{RECAST_cranmer} and aims to provide reintepretations as a service. Through a web interface, physicists would request a reinterpretation of a search, providing an alternative model, triggering a computational workflow executing the original analysis and delivering the recasted results. \Cref{sec:recast_implementation} discusses an attempt at fully preserving the search for electroweakinos presented in this work in the context of \textsc{Recast}. 

In many cases, the full precision of the original analysis pipeline is either not needed, or not accessible. As the full detector simulation requires access to the collaborations's detector description and is the most computationally expensive step in the signal pipeline, even when using fast simulations like \textsc{ATLFAST-II}, it is often approximated using simplified detector geometries and granularities. The most commonly used package for fast detector simulation outside of the collaboration is \textsc{Delphes}~\cite{Delphes:2009tx}. Other packages like \eg \textsc{Rivet}~\cite{Rivet1:2010ar,Rivet2:2019stt} approximate the detector response using dedicated 4-vector smearing techniques, assuming that the detector response roughly factorises into the responses of single particles. Internally, ATLAS also uses a dedicated framework for 4-vector smearing techniques, used in scenarios where other fast simulation techniques are still too expensive. \Cref{sec:truth_smearing} discusses these dedicated smearing functions further.

Similarly to the detector simulation, the analysis-specific event selection is also routinely approximated using different approaches. A number of public tools aiming to reimplement approximations of the event selections of various \gls{bsm} searches are available. Prominent examples include \textsc{CheckMate}~\cite{Checkmate2:2016npn,Checkmate:2013wra} and \textsc{MadAnalysis5}~\cite{MadAnalysis:2012fm}. ATLAS has internally maintained a similar catalogue of its \gls{susy} analyses and has published event selection snippets in C++ on \textsc{HEPData}~\cite{HEPData:2017ypu}. Recently, this package maintained by ATLAS, called \textsc{SimpleAnalysis}~\cite{simpleanalysis}, has been made publicly available, allowing the C++ snippets published to be run outside the collaboration.

Instead of trying to estimate the signal rates of a new signal model using \gls{mc} simulation and (reimplemented) analysis event selections, some reinterpretation efforts like \eg \textsc{SModelS}~\cite{SModelS1:2013mwa,SModelS2:2017neo} use \textit{efficiency maps} encoding the selection efficiency of the analysis as a function of some of the analysis observables (typically the sparticle masses). Such efficiency maps are routinely published by the ATLAS \gls{susy} searches on \textsc{HEPData}, and allows for efficient reinterpretations as long as the signal efficiencies mostly depend on the signal kinematics and are largely independent from the specific details of the signal model~\cite{SModelS1:2013mwa}. For the analysis presented in the previous part of this work, the efficiency maps and further analysis data products are available at \reference\cite{HEPdata_1Lbb}. 

\section{Public full likelihood}\label{sec:full_likelihood}

The likelihood is arguably one of the most information-dense and thus valuable data products of an analysis. Without precise knowledge of the exact likelihood of the original analysis, approximations need to be made for the statistical inference \eg in terms of correlations between event rate estimates as well as the treatment of uncertainties. Recently, ATLAS has started to publish full analysis likelihoods built using the \textsc{HistFactory} \gls{pdf} template introduced in~\cref{ch:statistics}~\cite{ATL-PHYS-PUB-2019-029}. This extraordinary step towards more open and reproducible science has been praised by the theory community~\cite{REINP:2020pec} as it allows for considerably more trustful reinterpretations. This effort has been facilitated by the development of \texttt{pyhf} in conjunction with the introduction of a \texttt{JSON} specification fully describing the \textsc{HistFactory} template. As a pure-text format, the \texttt{JSON} likelihoods are human- and machine-readable, highly compressible and can easily be put under version control, all of which are properties that make them ideal for long-term preservation. 

The full likelihood (in \texttt{JSON} format) of the search for electroweakinos presented in the previous part of this work has been published~\cite{fullLH_1Lbb} and is not only heavily used in the following chapters, but also in various analysis reinterpretation and combination efforts currently ongoing in ATLAS. Several efforts outside of the ATLAS collaboration have already included the analysis likelihood into their reinterpretations, \eg \textsc{SModelS}~\cite{SModelS_pyhf:2020grj} and \textsc{MadAnalysis}~\cite{Goodsell:2020ddr,Fuks:2021wpe} both reporting significant precision improvements through the use of the full likelihood (as opposed to approximating the statistical model). Furthermore, the full likelihood of the search presented herein has recently been used to demonstrate the concept of scalable distributed statistical inference on \glspl{hpc}~\cite{Feickert:2021sua}. Through the \texttt{funcX} package~\cite{chard20funcx}, \texttt{pyhf} is used as a highly scalable \textit{function as a service} to fit the entire signal grid of 125 signal points with a wall time of $\SI{156}{\second}$ using 85 available worker nodes\footnote{Theses benchmarks use \texttt{pyhf}'s \textsc{NumPy} backend and \textsc{SciPy} optimiser, which does have a slower log-likelihood minimisation time than \eg \textsc{PyTorch} coupled with \textsc{SciPy}, as will be shown in \cref{sec:cpu_performance}.}.

\section{Full analysis preservation using containerised workflows}\label{sec:recast_implementation}

For an analysis to be fully re-usable under the \textsc{Recast} paradigm, the signal pipeline of the original analysis (see \cref{fig:pipeline_analysis}) needs to be preserved such that it can be re-executed on new inputs. As typically only the processing steps after the event reconstruction are analysis-specific, it is sufficient to preserve this part of the signal pipeline. Processing steps preceding the calibration and selection of physics objects only involve the central ATLAS production system and result in \textit{derived analysis object data} formats that are used by analyses. These processing steps are preserved using centrally provided infrastructure.

   In the following, the term \textit{analysis pipeline} will refer to the analysis-specific data processing steps that are not handled by the central ATLAS production system, typically starting with selection of events in the \textit{derived analysis object data} format\improvement{cite} that have passed the reconstruction step in~\cref{fig:pipeline_analysis}. Preserving the analysis pipeline not only needs preservation of the full software environment for the different data processing steps, but also knowledge about the correct usage of the software through parameterised job templates together with a workflow graph connecting the different steps.

\subsection{Software preservation}

As much of the software is only tested, validated and deployed on a narrow set of architectures and platforms, the full software environment defining an analysis pipeline not only includes the original analysis-specific code used for object definitions, calibrations, event selection and statistical inference, but also the operating system used and a number of low-level system libraries that the applications depend upon. This can be achieved through the use of \textit{Docker containers}~\cite{docker,Binet:2134524} that---except for the operating system kernel---are able to package the full software environment, including a layered file system, the operating system as well as the actual application and all of its dependencies in a portable data format. As opposed to full virtualisation, Docker containers do not rely on hardware virtualisation but share the operating system kernel with host. Docker containers thus only interact with the host through system calls to the Linux kernel~\cite{ATL-PHYS-PUB-2019-032}\improvement{different source?} via a highly stable interface. This makes Docker containers a well-suited solution for deploying isolated applications on a heterogeneous infrastructure.

Due to the software structure of the analysis presented in this work, a containerisation requires a total of three container images spanning the following processing steps:
\begin{itemize}
	\item Event selection and physics object calibration: this step reads events in the \textit{derived analysis object data} format and produces flat \textsc{ROOT} files.
	\item Generation of expected signal rates: the histogram-building features of \textsc{HistFitter} are exploited to generate the necessary signal histograms in the relevant selections including all systematic variations. The histograms are subsequently converted into a \textsc{JSON} patch file that can be used to patch the partial likelihood.
	\item Statistical inference: although the original analysis used \textsc{HistFitter} for the statistical inference, the \textsc{Recast} implementation uses the \texttt{pyhf}-implementation of the \textsc{HistFactory} models in order to benefit from the possibility of using a partial \texttt{JSON} likelihood to preserve background and data rates. The \textsc{HistFitter} and \texttt{pyhf} implementations of the statistical inference have been shown to produce exactly the same results up to machine precision.
\end{itemize}

The first Docker image is based on a \textit{base image} providing a fixed ATLAS software release including all dependencies, expanded with the relevant analysis software. The second docker image uses the \textsc{ROOT} installation version originally used in the analysis, provided as part of a suitable ATLAS software release. The last image is based on a \texttt{pyhf} base image containing the \texttt{pyhf} release version used when validating the two \textsc{HistFactory} implementations against each other in the context of the analysis. All docker images are subject to version control and continuous integration, such that changes to the underlying software environment can be tracked and tagged. This allows for a consistent preservation of multiple versions of the analysis pipeline. \improvement{provide dockerfiles in appendix}

\subsection{Processing steps preservation}

Preserving the software environment is not sufficient, as detailed instructions on how to use it have to be given to the user. This is achieved through parameterised job templates that specify the precise commands and arguments required to re-execute the analysis code for specific processing steps. As re-executing the analysis pipeline using different signal models involves varying input parameters, all job template parameters are exposed to the user. Within \textsc{Recast}, the job templates are formulated using the YAML format. 

User-specifiable arguments and inputs to the event selection and physics object calibration step include the actual reconstructed \gls{mc} events in \textit{derived analysis object data} format, obtained through the central ATLAS production system, as well as corresponding files necessary for the pile-up correction in \gls{mc}. In addition, the signal process cross section as well as \gls{mc} generator-level efficiencies need to be given for correct normalisation the estimated signal rates to the integrated luminosity of the full Run~2 dataset. For each new signal model to be tested, three \gls{mc} samples need to be provided, generated with specific pile-up profiles close to the pile-up profile in data during the 2015--2016, 2017 and 2018 data-taking periods, respectively\footnote{This allows to have pile-up weights close to unity, avoiding unnecessary statistical dilution.}. In all three jobs, the events processed are weighted according to the integrated luminosity of the data-taking period they represent within the full Run~2 dataset. A subsequent \textit{merging} step uses the same docker image as the previous processing step, and serves to merge the three produced outputs into a single \textsc{ROOT} file that can be read by the subsequent step.

Apart from the merged \textsc{ROOT} output file produced in the previous step, the generation of the expected signal rates in a \texttt{JSON} patch format requires only one additional input---a \texttt{JSON} file containing theory uncertainties on the expected signal rates. These are optional and do not have to be specified if deemed to be negligible for the signal model to be tested.

The statistical inference steps requires the signal \texttt{JSON} patch from the previous step as well as the archived partial likelihood containing observed data as well as expected background rates including systematic variations thereof. 

\subsection{Workflow preservation}

Finally, the preserved processing steps need to be linked together, creating a parameterised workflow completely defining the analysis pipeline from centrally produced \gls{mc} datasets to the statistical inference results. Within \textsc{Recast}, this is achieved using the workflow description language \texttt{yadage}~\cite{yadage:2017frf}, capturing the full workflow in YAML format. The workflow uses the job templates and defines their processing order and dependencies. \Cref{fig:recast_workflow} shows a graph visualisation of the entire analysis pipeline, implemented in \textsc{Recast}. 

The \textsc{Recast} implementation of the analysis presented in this work has been validated against original analysis inputs. The expected and observed CL$_s$ values derived in the original analysis could be re-derived using the containerised workflow implementation. On a non-isolated CPU, the full preserved analysis pipeline for a single signal model can be executed within 1 hour. Due to the highly portable nature of the containerised workflow, the pipeline can easily be run in a distributed setup, allowing scalable reinterpretations at full analysis precision. 


 \begin{sidewaysfigure}[ht]
	\centering\includegraphics[width=\textwidth]{yadage_workflow_instance}
	\caption{Graph of the workflow as specified for the analysis pipeline. The containerised processing steps are represented as blue rectangular nodes, while input parameters, input files and outputs are shown as red oval nodes. The workflow is comprised of four processing steps: \texttt{selection\_stage\_mc16(a,d,e)}, \texttt{merging\_stage}, \texttt{workspace\_creation\_stage} and \texttt{fitting\_stage}. The first two steps perform the object calibration, event selection and merging of the three \gls{mc} datasets representing the three data-takin periods 2015--2016, 2017 and 2018. The latter two steps implement the generation of the signal \texttt{JSON} patch as well as the final statistical inference. Compared to \cref{fig:pipeline_analysis} the first two steps implement the \textit{signal analysis} part, while the latter two steps implement the \textit{statistical inference} deriving the final results.} 
	\label{fig:recast_workflow}
\end{sidewaysfigure}

\section{Simplified analysis preservation}\label{sec:simplified_preservation}

A full preservation of the entire analysis pipeline is highly desirable as it allows for a maximum precision reinterpretation of the original analysis in a new, promising signal model. As the full detector simulation needs a significant amount of CPU resources in addition to the non-negligible wall time of the actual preserved analysis pipeline, this approach can only be used on a limited set of models. In large-scale reinterpretations over high-dimensional parameter spaces, the amount of unique models that need to be sampled and investigated using the analysis is too high to employ the fully preserved analysis pipeline. In order to significantly reduce the wall time needed for passing through the analysis pipeline, a number of approximations and simplifications have to be made. 

In the following chapters, two major simplifications are discussed, targeting both the \textit{signal pipeline} as well as the \textit{statistical inference} blocks in~\cref{fig:pipeline_analysis}. \Cref{ch:simplify} introduces a procedure for building simplified likelihoods out of the published full likelihoods of ATLAS SUSY searches in order to significantly lower the wall time needed for running statistical fits in an analysis. \Cref{ch:pmssm} discusses an approach to approximate the \textit{signal pipeline} preceeding the statistical inference by resorting to truth-level analysis and approximating the detector response using dedicated smearing functions instead of running the full detector simulation. Both approximations are finally combined into a \textit{simplified analysis pipeline} and applied on a set of \gls{susy} models sampled from the \gls{pmssm}. 


