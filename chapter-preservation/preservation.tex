%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Analysis Preservation *********
%*******************************************************************************

\chapter{Preservation and reusability}\label{ch:preservation}

\graphicspath{{chapter-preservation/Figs/Vector/}{chapter-preservation/Figs/}}

Particle physics experiments such as the \gls{lhc} experiments are designed to collect physics data over several decades and operate at scales and complexities that make an independent and complete replication unfeasible or even a futile endeavour~\cite{open_is_not_enough}.
Due to their uniqueness, the data taken at these experiments and the physics results derived are highly valuable and challenge the scientific method from a reproducibility and reusability point of view~\cite{open_is_not_enough}.

In the following, reusability problems directly related to the computational analysis of a given dataset\footnote{This is in contrast to also considering the actual collection of data. As such, the implementation of a computational analysis of a dataset can, in the following, be seen as an experimental setup that needs to be preserved and reusable.} are discussed, and approaches taken in view of analysis preservation and reusability are presented.
This chapter starts with a brief motivation for \textit{reinterpretations}, \ie reusing an analysis in light of additional signal models, which is followed by a description of the main ingredients required.
The remaining sections discuss three separate efforts aiming to improve the reusability of the \onelepton analysis. All three efforts are not only relevant in the scope 	 this thesis, but also for reinterpretation activities currently ongoing within ATLAS.

\section{The case for reinterpretations}\label{sec:reinterpretations}

\subsection{Motivation}
Designing and performing searches for \gls{bsm} physics requires a substantial amount of person-power and computing resources. As laid out in detail in \cref{part:simplified_model_analysis} of this thesis, an analysis generally aims to design signal regions in which a given \gls{bsm} signal can be efficiently discriminated against \gls{sm} background. Although the careful design of such regions already requires a significant amount of resources, it constitutes only a fraction of the work necessary for concluding the search. 
Contributions in the signal regions from \gls{sm} processes need to be estimated, usually requiring expensive \gls{mc} simulations and the development of background estimation strategies. Systematic uncertainties arising from numerous sources need to be considered and their impact estimated. 
For the \gls{bsm} signal, a similar processing pipeline involving \gls{mc} simulation, event reconstruction and event selection including uncertainties needs to be executed.
Furthermore, recorded data also has to be reconstructed and processed through the analysis-specific event selection.
Only after the expected and observed event rates in all regions are known, the statistical evaluation can be performed, and the final analysis results, \eg, quantifying excesses in data or setting limits on model parameters, can be determined.
\Cref{fig:pipeline_analysis} illustrates the formal structure of such an analysis, consisting of three main processing pipelines; a \textit{background pipeline}, a \textit{signal pipeline} and a \textit{data pipeline}; followed by the \textit{statistical inference}.

Due to the substantial amount of resources necessary for developing and performing an analysis, it is not feasible to develop dedicated searches for every possible \gls{bsm} scenario.
Instead, analyses are typically only interpreted in a finite set of models with a small number of free parameters that need to be varied.
Still, it is likely that a given analysis is sensitive to a variety of different \gls{bsm} scenarios not considered in the original publication. 
Consequently, it is not surprising that there is significant interest in the \gls{hep} community to reinterpret \gls{bsm} searches in different signal models. Reinterpretations of ATLAS searches for \gls{susy} are routinely performed by various reinterpretation efforts.
In the context of direct constraints on \gls{bsm} physics\footnote{As discussed to some extent in \cref{sec:shortcomings_sm}, indirect constraints on \gls{bsm} models can also come from \gls{sm} precision measurements.}, the search results published by the experimental collaborations represent the only windows into the \gls{lhc} data that are available to the wider \gls{hep} community.
Reinterpretations of \gls{bsm} searches are thus the only possibility to determine the direct implications of \gls{lhc} data for a broad range of models~\cite{reinterpretation_workshop}.

As will be discussed in detail in \cref{ch:pmssm}, reinterpretations are not only of interest for the wider \gls{hep} community, but also for the experimental collaborations themselves. Within the ATLAS Collaboration, reinterpretations of \gls{susy} searches in complete \gls{susy} models can, for example, serve as powerful tools to state a comprehensive summary of the overall sensitivity to more realistic supersymmetric models. As such, the efforts discussed in the remainder of this chapter, as well as in \cref{ch:simplify,ch:pmssm}, are not only relevant for the work presented in this thesis, but also reinterpretation efforts currently ongoing within the ATLAS Collaboration.
 
 \begin{figure}
	\centering\includegraphics[width=\textwidth]{pipeline}
	\caption{Full analysis workflow including the three main processing pipelines for deriving background and signal estimates as well as observed data counts. The outputs of the three processing pipelines are combined into a likelihood forming the basis for the statistical inference. In a \textsc{Recast} setup (details in the text), the estimated background rates and observed data counts are archived, and the signal pipeline is fully preserved, such that it can be re-executed with different inputs at any time. Figure created by the author but based on \mbox{\reference\cite{ATL-PHYS-PUB-2019-032}}.}
	\label{fig:pipeline_analysis}
\end{figure}

\subsection{Approaches for reinterpretations}

%Although of high interest, reinterpretations of searches are a daunting undertaking. Whilst analysis efforts typically maintain a comprehensive documentation of the methods and algorithms developed, the complexity of the software implementations may hide minute but crucial details, which can lead to a loss of knowledge concerning how the analysis results were derived. 

As the event selection of an analysis is fixed, the \textit{pre-fit} background estimates (\ie the estimated background rates before the background-only fit described in \cref{sec:results_background_only}) and observed data counts in the regions of interest of the analysis do not change.
The data and background pipelines shown in~\cref{fig:pipeline_analysis}, entering the statistical inference of the analysis only by means of event rates, can therefore be archived in a format that is significantly smaller than the original input data.
Hence, reinterpreting a search in the light of a new signal model requires the re-execution of only two of the main analysis ingredients with (partially) new inputs; the signal pipeline and the statistical inference.

Recently, it has become possible to preserve the partial analysis likelihood\footnote{As before, this only refers to likelihoods built using the \textsc{HistFactory} template.} built from the background estimates and observed data in a pure-text format~\cite{ATL-PHYS-PUB-2019-029}, including all nuisance parameters and auxiliary data. Once the signal estimates are known, a new full analysis likelihood can be built, and the viability of the new signal model can be tested with respect to the analysis in question. In \cref{fig:pipeline_analysis}, the preserved partial likelihood is indicated through a red rectangle. The pure-text format of the likelihood readily lends itself to publication of the likelihood, an effort that is further discussed in~\cref{sec:full_likelihood}.

Different approaches can be taken for rendering the signal pipeline reusable to the extent that event rate estimates for new \gls{bsm} scenarios of interest can be derived. 
Whilst analysis efforts typically maintain a comprehensive documentation of the methods and algorithms developed, the complexity of the software implementations may hide minute but crucial details, which can lead to a loss of knowledge concerning how the analysis results were derived.
Manifestly the most precise approach thus involves executing the original analysis software, but using a different \gls{bsm} model as input.
This requires the preservation of the entirety of the original software environment, including the exact workflows in a parameterised form, and therefore constitutes the most technologically demanding and involved approach. A framework designed to facilitate such an effort, called \textsc{Recast}, was originally proposed in \reference\cite{RECAST_cranmer} and is currently under development. It aims to provide the cyber-infrastructure needed for offering \textit{reinterpretations as a service}. 
Physicists, wishing to reinterpret a search with \textsc{Recast}, would provide an alternative \gls{bsm} model through a web interface and trigger an ATLAS-internal computational workflow that would re-execute the original analysis using the new signal inputs, ultimately delivering the \textit{recasted} results. An attempt to fully preserve the \onelepton search using the \textsc{Recast} paradigm is discussed in \cref{sec:recast_implementation}. 

As the details of the existing \textsc{Recast} implementations of ATLAS searches for \gls{susy} are not publicly available, but only meant to be interacted with through a formal \textsc{Recast} request, the exact implementation of the analysis selection is in general not available outside the ATLAS Collaboration\footnote{As a matter of fact, the exact implementation is often not fully re-executable outside the small original analysis team to begin with. Recastable analyses are therefore already important for various efforts (some of which are discussed in the following) within the ATLAS Collaboration itself.}. 
For this reason, a number of public tools aiming to reimplement an approximated version of the event selections of a number of \gls{bsm} searches at the \gls{lhc} are available.
Prominent examples include \textsc{CheckMate}~\cite{Checkmate2:2016npn,Checkmate:2013wra} and \textsc{MadAnalysis5}~\cite{MadAnalysis:2012fm}.
ATLAS has internally maintained a similar catalogue of its \gls{susy} analyses and is publishing event selection snippets in \Cpp for many \gls{susy} searches on \textsc{HEPData}~\cite{HEPData:2017ypu}, a repository for high energy physics data.
Recently, this package maintained by ATLAS, called \textsc{SimpleAnalysis}~\cite{simpleanalysis}, has been made publicly available, allowing the \Cpp snippets to be executed outside the collaboration.

A crucial step, necessary for achieving a reliable reimplementation of the signal pipeline, is the detector simulation.
Executing the full detector simulation requires access to the collaboration's detector description and is computationally expensive, disfavouring\footnote{This is especially true for reinterpretation efforts outside the ATLAS Collaboration, which, for reasons not discussed herein, cannot make use of the collaboration's detector description.} its usage in the context of large-scale reinterpretations over a large set of models.
For this reason, it is often approximated using simplified detector geometries and granularities.
The most common package for a fast detector simulation outside of the ATLAS Collaboration is \textsc{Delphes}~\cite{Delphes:2009tx}, which is used in, \eg, \textsc{CheckMate} and \textsc{MadAnalysis5}.
Other packages like, \eg, \textsc{Rivet}~\cite{Rivet1:2010ar,Rivet2:2019stt} approximate the detector response using dedicated four-vector smearing techniques, assuming that the detector response roughly factorises into the responses of single particles.
Internally, the ATLAS Collaboration also maintains a dedicated framework for four-vector smearing, used in scenarios where other fast simulation techniques are still too expensive.
As they will be heavily exploited in \cref{ch:pmssm}, these dedicated smearing functions are further discussed in \cref{sec:truth_smearing}.

Finally, instead of attempting to estimate the signal rates of a new model using \gls{mc} simulation and (reimplemented) analysis event selections, some reinterpretation efforts, as for example \textsc{SModelS}~\cite{SModelS1:2013mwa,SModelS2:2017neo}, use \textit{efficiency maps} encoding the selection and acceptance efficiencies of the analysis as a function of the model parameters (typically the sparticle masses in the case of \gls{susy} searches) and analysis selections.
Such efficiency maps are routinely published on \textsc{HEPData} by ATLAS searches for \gls{susy}, and allow for efficient reinterpretations, as long as the signal efficiencies mostly depend on the signal kinematics and are largely independent from the specific details of the signal model~\cite{SModelS1:2013mwa}. For the \onelepton search presented herein, the efficiency maps, including additional analysis data products, are available at \reference\cite{HEPdata_1Lbb}. 

\section{Public full likelihood}\label{sec:full_likelihood}

The likelihood is arguably one of the most information-dense and important data products of an analysis.
If the exact likelihood function of the original analysis is not known in reinterpretation efforts\footnote{Up until recently, the exact likelihood function was not part of the data products published by ATLAS searches for \gls{susy}, hence approximations of the statistical models were naturally a crucial part of most reinterpreation efforts outside the collaboration.}, approximations need to be made for the statistical inference, \eg, in terms of the correlations between event rate estimates as well as the treatment of uncertainties.
Recently, ATLAS has started to publish full analysis likelihoods built using the \textsc{HistFactory} \gls{pdf} template~\cite{ATL-PHYS-PUB-2019-029}.
This effort has been facilitated by the development of \texttt{pyhf}~\cite{pyhf_joss,pyhf} (cf. \cref{sec:likelihood_function}), in conjunction with the introduction of a \texttt{JSON} specification fully describing the \textsc{HistFactory} template.
As a pure-text format, the \texttt{JSON} likelihoods are human- and machine-readable, highly compressible and can easily be put under version control, all of which are properties that make them suitable for long-term preservation, a property that is a crucial condition for reinterpretations.

The full likelihood of the \onelepton search is publicly available at \reference\cite{fullLH_1Lbb} and is not only heavily used in the following chapters, but also in various analysis reinterpretation and combination efforts currently ongoing in the ATLAS Collaboration.
Several efforts outside of the ATLAS Collaboration have already included the analysis likelihood into their reinterpretations, and the \textsc{SModelS} and \textsc{MadAnalysis5} Collaborations have both reported significant precision improvements through its use~\cite{SModelS_pyhf:2020grj,Goodsell:2020ddr,Fuks:2021wpe}. Furthermore, the full likelihood of the search presented herein has recently been used to demonstrate the concept of scalable distributed statistical inference on \glspl{hpc}~\cite{Feickert:2021sua}.
Through the \texttt{funcX} package~\cite{chard20funcx}, \texttt{pyhf} is leveraged as a highly scalable \textit{function as a service} to fit the entire \onelepton signal grid of 125 signal points with a wall time of $\SI{156}{\second}$ using 85 available worker nodes\footnote{These benchmarks use \texttt{pyhf}'s \textsc{NumPy} backend and \textsc{SciPy} optimiser, a combination that has a slower log-likelihood minimisation time than \eg \textsc{PyTorch} coupled with \textsc{SciPy}, as will be shown in \cref{sec:cpu_performance}. In that sense, the performance quoted in \reference\cite{Feickert:2021sua} is slightly conservative.}.

\section{Full analysis preservation using containerised workflows}\label{sec:recast_implementation}

For an analysis to be fully reusable under the \textsc{Recast} paradigm, the signal pipeline of the original analysis (cf. \cref{fig:pipeline_analysis}) needs to be preserved such that it can be re-executed on new inputs.
As typically only the processing steps after the event reconstruction are analysis-specific, it is sufficient to preserve this part of the signal pipeline.
Processing steps including and preceding the event reconstruction only involve the central ATLAS production system, introduced in \cref{sec:mc_simulation}, and result in an ATLAS-internal data format serving as input for physics analyses. These processing steps are preserved using centrally provided ATLAS infrastructure and thus do not need to be within the scope of the preservation discussed in the following.

 In the following, the term \textit{signal analysis} will, as indicated in \cref{fig:pipeline_analysis}, refer to the analysis-specific processing steps that are not handled by the central ATLAS production system, typically starting with the selection of events that have passed the reconstruction step, provided in the aforementioned internal data format. Preserving the signal analysis not only needs preservation of the full software environment required for the different processing steps, but also knowledge of the correct usage of the software through parameterised job templates together with a workflow graph connecting the different processing steps. A graph representation of the entire analysis, implemented in \textsc{Recast}, is shown in \cref{fig:recast_workflow}.

\subsection{Software preservation}

As much of the software is only tested, validated and deployed on a narrow set of architectures and platforms, the full software environment defining an analysis pipeline not only includes the original analysis-specific code used for object definitions, calibrations, event selection and statistical inference, but also the operating system used, and a number of low-level system libraries that the applications depend upon.
Preserving the full software environment can be achieved through the use of \textit{Docker containers}~\cite{docker,Binet:2134524}, a technology that---except for the operating system kernel---packages the full software environment into a portable data format, including a layered file system, the operating system as well as the actual application and all of its dependencies.
As opposed to full virtualisation, Docker containers do not rely on actual hardware virtualisation but share the operating system kernel with the host, \ie the computing system that the containers are run on. As such, they only interact with the host through system calls to the Linux kernel~\cite{Binet:2134524}, offering a highly stable interface. This makes Docker containers a well-suited, lightweight solution for deploying isolated applications on a heterogeneous computing infrastructure.

Due to the specific software structure of the \onelepton search, a containerisation requires a total of three container images.
Two images contain the software necessary for performing the physics object calibrations and event selection, as well as the conversion of the information in a format that can be used by the downstream steps. The third image contains the software necessary for the statistical inference, relying on the \texttt{pyhf}-implementation of the \textsc{HistFactory} models in order to benefit from the possibility of using a partial \texttt{JSON} likelihood to preserve background and data rates.
%  
%\begin{itemize}
%	\item Event selection and physics object calibration: this step reads events in the ATLAS-internal analysis format and produces \textsc{ROOT} files with a flat structure to be used in the following.
%	\item Determination of expected signal rates in analysis regions: the histogram-building features of \textsc{HistFitter} are exploited to generate the necessary signal histograms in the relevant selections including all systematic variations. The histograms are subsequently converted into a \textsc{JSON} patch file that can be combined with the partial, preserved likelihood to create a full analysis likelihood function.
%	\item Statistical inference: although the original analysis used \textsc{HistFitter} for the statistical inference, the \textsc{Recast} implementation uses the \texttt{pyhf}-implementation of the \textsc{HistFactory} models in order to benefit from the possibility of using a partial \texttt{JSON} likelihood to preserve background and data rates. Studies have shown that the \textsc{HistFitter} and \texttt{pyhf} implementations of the statistical inference produce the same results (cf. \eg \reference\cite{ATL-PHYS-PUB-2019-029}).
%\end{itemize}

The Docker images are built from suitable base images containing the software environment used for deriving the published \onelepton search results, expanded with the relevant analysis software. All docker images are subject to version control and continuous integration, such that changes to the underlying software environment can be automatically tracked and tagged. This enables a consistent preservation of multiple versions of the analysis pipeline. \improvement{provide dockerfiles in appendix}

\subsection{Processing steps preservation}

Preserving the software environment is not sufficient, as detailed instructions on how to use it have to be given. This is achieved through parameterised job templates that specify the precise commands and arguments required to re-execute the analysis code for specific processing steps. As re-executing the analysis pipeline using different signal models involves varying input parameters, all job template parameters are exposed to the user. In \cref{fig:recast_workflow}, the parameterised job templates are shown as blue rectangles, while their input arguments and outputs are illustrated as red oval nodes. 
%In the following, a brief overview of the user-specifiable arguments and inputs to the job templates is given.

The user-specifiable arguments for the event selection and physics object calibration step require the actual reconstructed events in the aforementioned ATLAS-internal format as input, as well as corresponding inputs necessary for the pile-up correction. The cross section of the signal process in question needs to be provided together with any generator-level efficiencies. Furthermore, a separate input file containing the theory uncertainties on the expected signal rates needs to be given. Finally, the statistical inference step, generating a new full analysis likelihood and performing the necessary hypothesis tests, requires the partial likelihood previously discussed as input.

%The event selection and physics object calibration step require the actual reconstructed \gls{mc} events in the aforementioned ATLAS-internal format as input (referred to as `\texttt{input\_mc16(a,d,e)}' in \cref{fig:recast_workflow}), obtained through the central ATLAS production system, as well as corresponding files necessary for the pile-up correction in \gls{mc} (`\texttt{prwfile\_mc16(a,d,e)}'). 
%For each new signal model to be tested, three \gls{mc} samples need to be provided, generated with specific pile-up profiles close to the pile-up profile in data during the 2015--2016, 2017 and 2018 data-taking periods, respectively\footnote{This allows to have pile-up weights relatively close to unity, avoiding unnecessary statistical dilution.}. 
%In all three jobs, the events processed are weighted according to the integrated luminosity of the data-taking period they represent within the full Run~2 dataset.
%For the correct normalisation of the estimated signal rates to the integrated luminosity of the full Run~2 dataset, the signal process cross section (\texttt{xsec}), as well as \gls{mc} generator-level efficiencies (\texttt{filter\_eff}) need to be given. 
%A subsequent \textit{merging} step relies on the same docker image as the previous processing step, and serves to merge the three produced outputs into a single \textsc{ROOT} file containing bin-wise expected signal rates and experimental uncertainties.
%
%In addition, a \texttt{JSON} file containing theory uncertainties on the expected signal rates can be provided (\texttt{signal\_theory\_uncertainties}). These are optional and do not have to be specified if deemed to be negligible for the signal model under consideration.
%
%The statistical inference step requires, as external input, the archived partial likelihood containing observed data as well as expected background rates including systematic variations thereof (\texttt{partial\_likelihood}). This step generates a new full analysis likelihood and performs the necessary hypothesis tests.  

\subsection{Workflow preservation}

 \begin{sidewaysfigure}
	\centering\includegraphics[width=1.0\textwidth]{yadage_workflow_instance}
	\caption{Graph of the workflow as specified for the analysis pipeline. The containerised processing steps are represented as blue rectangular nodes, while input parameters, input files and outputs are shown as red oval nodes. The workflow is comprised of four processing steps: \texttt{signal\_analysis\_stage\_mc16(a,d,e)}, \texttt{merging\_stage}, \texttt{workspace\_creation\_stage} and \texttt{statistical\_inference\_stage}. The first two steps perform the object calibration, event selection and merging of the three \gls{mc} datasets representing the three data-taking periods 2015--2016, 2017 and 2018. The latter two steps implement the patching of the partial likelihood with the expected signal rates, as well as the final statistical inference. Compared to \cref{fig:pipeline_analysis} the first two steps implement the \textit{signal analysis} part, while the latter two steps implement the \textit{statistical inference} deriving the final results. Figured created using \textsc{GraphViz}~\cite{Gansner00anopen,neato}.} 
	\label{fig:recast_workflow}
\end{sidewaysfigure}


Finally, the preserved processing steps need to be linked together, creating a parameterised workflow completely defining the analysis pipeline, starting from centrally produced \gls{mc} datasets up to the statistical inference results. Within \textsc{Recast}, this is achieved using the workflow description language \texttt{yadage}~\cite{yadage:2017frf}, capturing the full workflow in \texttt{YAML} format. The workflow connects the job templates and defines their processing order and dependencies. In \cref{fig:recast_workflow}, it is indicated through the black arrows connecting the nodes of the graph.

The \textsc{Recast} implementation of the analysis presented in this work has been validated against original analysis inputs. The expected and observed CL$_s$ values derived in the original analysis were successfully re-derived using the containerised workflow implementation. On a non-isolated CPU, the full preserved analysis pipeline for a single signal model can be executed with a wall time of about $\SI{50}{\minute}$. Due to the highly portable nature of the containerised workflow, the pipeline can easily be run in a distributed setup, allowing scalable reinterpretations at full analysis precision. Although not explicitly used in the remainder of this thesis, the \textsc{Recast} implementation of the \onelepton search is crucial for the large-scale reinterpretation efforts in the \gls{pmssm} currently ongoing in ATLAS (and discussed to some extent in \cref{ch:pmssm}). In these efforts, the \textsc{Recast} implementation allows the systematic reinterpretation of the \onelepton search in any \gls{pmssm} model of interest using the full analysis precision.




%\FloatBarrier

\section{Truth-level analysis}\label{sec:truth_analysis}

\graphicspath{{chapter-pmssm/Figs/Vector/}{chapter-pmssm/Figs/}}


A full preservation of the entire analysis pipeline, as discussed in the previous section, is highly desirable, since it allows for a maximum precision reinterpretation of the original analysis using a new \gls{bsm} model.
As the full detector simulation needs a significant amount of computing resources in addition to the non-negligible wall time of the actual preserved analysis pipeline, this approach can only be used on a limited set of models.
In large-scale reinterpretations over high-dimensional parameter spaces, the amount of models that need to be sampled and investigated using the analysis is too large to run the fully preserved analysis pipeline in every case.
In order to significantly reduce the number of models that need to be passed through the full analysis pipeline, a pre-sorting using a simplified analysis implementation can be exploited. Models that can be safely considered to be (non-)excluded based on this simplified analysis implementation consequently do not need to be evaluated at the full analysis precision, potentially saving a significant amount of computing resources.

In the following, two complementary approaches to analysis simplifications are discussed, targeting both the \textit{signal pipeline} as well as the \textit{statistical inference} blocks in~\cref{fig:pipeline_analysis}.
This section discusses the \textsc{SimpleAnalysis} implementation of the analysis, an approach implementing the signal pipeline at \textit{truth-level}, \ie using the generator-level objects without running a dedicated detector simulation. An approximation of the detector response using four-vector smearing techniques is discussed.

The second simplification is discussed in \cref{ch:simplify}, introducing a procedure for building simplified likelihoods from the full likelihoods of ATLAS searches for \gls{susy}, allowing a significant decrease of the wall time needed for the statistical inference. 
In \cref{ch:pmssm}, both approximations are combined and applied, in the context of the \onelepton search, on a set of \gls{susy} models sampled from the \gls{pmssm}.

%As discussed in~\cref{ch:preservation}, the reinterpretation of an analysis involves re-executing the analysis pipeline in order to derived signal rate estimates in all regions. In large-scale reinterpretations, running a \textsc{Recast} implementation on all signal models considered is not computationally feasible and instead a \textit{truth-level} analysis is first performed for all signal models sampled. Only models with uncertain exclusion at truth-level are processed through the computationally expensive full analysis chain implemented in \textsc{Recast}. The truth-level analysis skips the detector simulation and uses generator-level objects instead. Any detector-level effects and inefficiencies will thus not be reflected in truth-level observables. In order to reproduce the kinematic distributions observed in the full analysis (using reconstruction-level objects), a dedicated \textit{truth smearing}---discussed in detail in~\cref{sec:truth_smearing}---is applied.

\subsection{Truth-level selection}\label{sec:truth_selection}

 \begin{figure}
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324_noLabel_noOR/700_150/lep1Pt_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324_noLabel_noOR/700_150/jet1Pt_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\caption{Impact of the overlap removal (OR) procedure at truth-level illustrated in the lepton and leading jet transverse momenta distributions. The truth-distributions with (blue) and without (green) overlap removal are compared with a reconstruction-level (orange) distribution. The exemplary benchmark signal point with \mbox{$m(\charg$/$\neutr), m(\lsp) = 700, \SI{150}{\GeV}$} is shown in both plots. Both truth-level distributions are shown after smearing. All distributions are shown in a loose preselection requiring an electron or a muon, $\met>\SI{50}{\GeV}$, $\mt > \SI{50}{\GeV}$, and 2--3 jets, two of which need to be \textit{b}-tagged.}
	\label{fig:overlap_removal_truth}
\end{figure}

All signal and control regions considered in the original \onelepton search are implemented at truth-level using the \textsc{SimpleAnalysis} framework.
The exact implementation has been published, together with the previously discussed efficiency maps and analysis likelihod, as part of the auxiliary analysis data at \reference\cite{HEPdata_1Lbb}.
In fact, the \textsc{SimpleAnalysis} implementation of the search was already used in~\cref{ch:uncertainties} for the derivation of some of the theory uncertainties.

The truth-level implementation explicitly specifies all object definitions introduced in~\cref{sec:object_definitions}, even though some of them, like the lepton isolation, are technically not well-defined at truth-level.
The four-vector smearing described in the following, is in many cases, however, implemented as a function of said object definitions and hence still allows to consider them to some extent.
Additionally, as discussed in~\cref{sec:reinterpretations}, the full specification of the original analysis event selection, including all object definitions, allows for more straightforward reinterpretations by efforts outside of the ATLAS Collaboration that generally do not have access to the original analysis software.

Following the object definitions, an overlap removal procedure, adhering to the same prescription for the reconstruction-level\footnote{The term \textit{reconstruction-level} here refers to distributions obtained with \gls{mc} simulated datasets for which either the full detector simulation using \textsc{Geant4}, or the \textsc{ATLFAST-II} fast simulation have been run with subsequent object reconstruction.} analysis (cf.~\cref{sec:overlap_removal}), is performed. The truth-level overlap removal especially also relies on the same shrinking cone definitions used at reconstruction-level.
Since tracking information is not available at truth-level, the overlap removal step removing electrons sharing a track with a muon is approximated by using a distance parameter of $\upDelta R = 0.01$ between the objects.
Although often neglected\footnote{The overlap removal procedures in ATLAS \gls{susy} searches tend to be quite intricate, rendering them non-trivial to re-implement without ATLAS and analysis-specific knowledge.} in reinterpretation efforts outside of the collaboration, the correct implementation of the overlap removal procedure employed in the original analysis is crucial to reproduce the signal estimates of the original analysis.
\Cref{fig:overlap_removal_truth} illustrates this by showing the lepton and leading jet $\pt$ distributions of an exemplary signal point in configurations with and without overlap removal at truth-level, and comparing it with the distributions obtained at reconstruction-level.
Not implementing the overlap removal procedure of the original \onelepton search, results in many truth-level events not passing the analysis selections. This is due to additional truth-level objects in the final state that would otherwise have been removed through the overlap removal. 

Finally, the exact implementation of all analysis observables is explicitly given, followed by the definition of all control and signal regions.

\subsection{Truth smearing}\label{sec:truth_smearing}

The general assumption of the truth smearing discussed herein is that the detector response roughly factorises into the responses of single particles.
This allows to use the ATLAS detector performance results for constructing detector response maps parameterised in different observables for each physics object.
Detector response maps include object reconstruction and identification efficiencies as well as scale factors to correct for differences between \gls{mc} simulation and observed data.
Likewise, effects from the finite resolution of energy measurements in the detector are modelled through energy or momentum resolution maps. In the following, the four-vector components of electrons, muons, jets and $\etmiss$ are smeared. 
%The implementation of the smearing functions is internal to ATLAS and originates predominantly from various upgrade studies.

In the case of truth electrons, the identification efficiencies considered are parameterised in $\vert\eta\vert$ and $\pt$~\cite{PERF-2017-01}. In $\vert\eta\vert$, nine fixed-width bins are used to parameterise the identification efficiency. In $\pt$, six bins are implemented and a linear interpolation between two adjacent $\pt$-bins is employed to get the efficiency for the  $\pt$ of each truth electron.
Different efficiency maps exist for the different working points of the likelihood-based identification discriminant introduced in \cref{sec:reco_electrons}~\cite{PERF-2017-01}.
The probability of finding a fake electron in a truth jet is estimated through a similar two-dimensional map depending on the truth jet $\eta$ and $\pt$, again relying on fixed-width bins in $\vert\eta\vert$ and a linear interpolation in $\pt$. %~\cite{PERF-2017-01}
The range of the $\pt$ interpolation for identification efficiencies and fake rates extends from $\SI{7}{\GeV}$ to $\SI{120}{\GeV}$, covering the majority of all electrons in the analysis.
If the truth $\pt$ of the electron is outside of this range, the identification efficiency and fake rate from the respective bound of the corresponding $\vert\eta\vert$-bin are taken.
The probability of misidentifying an electron as a photon is estimated with different fixed values for the barrel and end-cap regions~\cite{PERF-2017-02}.
Finally, the transverse energy of the electron is smeared with a random number drawn from a Gaussian distribution with standard deviation corresponding to the $\vert\eta\vert$- and $\pt$-dependent energy resolution, measured in $Z\to ee$ and $J/\Psi\to ee$ events~\cite{PERF-2017-03}.

For truth muons, the identification efficiencies are also parameterised in $\vert\eta\vert$ and $\pt$~\cite{Aad:2020gmm}. Different efficiency maps exist again for the different identification working points (cf. \cref{sec:reco_muon})~\cite{Aad:2020gmm}. Similar to truth electrons, the  $\pt$ of the muon is smeared using a Gaussian distribution with standard deviation corresponding to the momentum resolution. The momentum resolution of combined truth muons ($\sigma_\mathrm{CB}$) is computed from the resolutions in the inner detector ($\sigma_\mathrm{ID}$) and the muon spectrometer ($\sigma_\mathrm{MS}$) as
\begin{equation}
	\sigma_\mathrm{CB} = \frac{\sigma_\mathrm{ID}\sigma_\mathrm{MS}}{\sqrt{\sigma_\mathrm{ID}^2 + \sigma_\mathrm{MS}^2}},
\end{equation}
where $\sigma_\mathrm{ID}$ and $\sigma_\mathrm{MS}$ are parameterised in $\vert\eta\vert$ and $\pt$ and measured in $Z\to \mu\mu$ and $J/\Psi\to \mu\mu$ events~\cite{PERF-2015-10}.

The transverse momentum of truth jets is smeared using a Gaussian with standard deviation equal to the \gls{jer}, provided in a map parameterised in five bins in $\vert\eta\vert$, ranging from $\vert\eta\vert = 0$ to $\vert\eta\vert = 4.5$. The jet energy resolutions are measured in dijet events~\cite{Aad:2020flx} and provided as parameterisations of a noise $N$, stochastic $S$ and constant $C$ term for each of the seven bins in $\vert\eta\vert$, such that the resolution can be computed as
\begin{equation}
	\frac{\sigma(\pt)}{\pt} = \frac{N}{\pt}\oplus\frac{S}{\sqrt{\pt}}\oplus C.
\end{equation}
Only truth jets with $\SI{10}{\GeV} < \pt < \SI{1.5}{\TeV}$ are smeared. For truth jets with $\pt > \SI{20}{\GeV}$, the flavour tagging efficiency is considered through efficiencies parameterised in $\vert\eta\vert$ and $\pt$. Different flavour tagging efficiency maps are available for the different \textsc{MV2c10} efficiency working points (introduced in~\cref{sec:object_definitions}). All flavour tagging efficiencies are measured in fully reconstructed simulated $\ttbar$ events~\cite{FTAG-2018-01}.

Finally, the smeared missing transverse energy is computed by considering the transverse momenta of all smeared truth objects in the event. An approximation for the track soft term is estimated through resolution measurements from $Z\rightarrow \ell\ell$ events~\cite{ATLAS-CONF-2018-023}, allowing to infer a distribution of the mean soft term projected in the direction longitudinal to the total transverse momentum of all hard objects in an event, $\makemebold{p}_\mathrm{T}^\mathrm{hard}$. The measured resolution parallel and perpendicular to $\makemebold{p}_\mathrm{T}^\mathrm{hard}$ is then used to smear the nominal soft track value.

  \begin{figure}
	\centering
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/met_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/mt_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/mct_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/mbb_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/lep1Pt_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/jet1Pt_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/mlb1_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.47\linewidth}
		\centering\includegraphics[width=\textwidth]{20210324/700_150/nBJet30_C1N2_Wh_hbb_700p0_150p0_smeared.pdf}
	\end{subfigure}\hfill
	\caption{Comparisons of the kinematic distributions of relevant observables at (smeared) truth- and reconstruction-level. An exemplary benchmark signal point with electroweakino mass parameters \mbox{$m(\charg$/$\neutr), m(\lsp) = 700, \SI{150}{\GeV}$} is shown. The ratio pad shows the ratio of smeared and unsmeared truth-level distributions (blue and green) to reconstruction-level distributions (orange). Only \gls{mc} statistical uncertainties are included in the error bars. All distributions are shown in a loose preselection requiring exactly one electron or muon, $\met>\SI{50}{\GeV}$, $\mt > \SI{50}{\GeV}$, and 2--3 jets, two of which need to be \textit{b}-tagged. The latter requirement is dropped for the \textit{b}-jet multiplicity distribution.}
	\label{fig:smearing_preselection}
\end{figure}
 
 \begin{figure}
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{yields_SR-LM_unsmeared}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{yields_SR-LM_smeared}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{yields_SR-MM_unsmeared}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{yields_SR-MM_smeared}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{yields_SR-HM_unsmeared}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering\includegraphics[width=\textwidth]{yields_SR-HM_smeared}
	\end{subfigure}
	\caption{Comparison of the expected event rates at truth- and reconstruction-level before (left) and after (right) truth smearing. From top to bottom, the SR-LM, SR-MM and SR-HM signal regions are shown, with cumulative (integrated) $\mct$ bins. Every single point in the scatter plots represents a single signal model considered in the \onelepton search. Uncertainty bars include \gls{mc} statistical uncertainties.}
	\label{fig:smearing_signal_regions}
\end{figure} 
 
\section{Validation of the truth-level analysis}

\subsection{Validation in the loose preselection}

 The performance of the truth smearing is illustrated in~\cref{fig:smearing_preselection} in a loose preselection for an exemplary benchmark signal point.
 The loose preselection applied requires a final state with an electron or muon, $\met>\SI{50}{\GeV}$, $\mt > \SI{50}{\GeV}$, and 2--3 jets, two of which need to be \textit{b}-tagged.
 The reconstruction-level distributions are compared with the truth-level distributions before and after truth smearing. It can be observed that the truth smearing noticeably improves the agreement between the truth- and reconstruction-level distributions.
 While the lepton and jet reconstruction and identification efficiencies are---due to their dependence on $\eta$, $\pt$ and individual identification and isolation working points---crucial for the overall agreement in shape, especially at low $\pt$, the inclusion of flavour-tagging efficiencies significantly improves the overall agreement in normalisation.
 
Although some minor differences remain, a good agreement is observed across the relevant kinematic distributions at loose preselection level.
Most of the differences remaining between smeared truth-level and reconstruction-level distributions in individual bins are well within the \gls{mc} statistical uncertainties, arising from the relatively limited \gls{mc} statistics available.

 
 \subsection{Validation in the signal regions}
 
 As the expected signal rates in the signal regions are ultimately what is entering the statistical inference, it is important that the good agreement observed at preselection is still present in the kinematically tighter selections of the signal regions.
 Additionally, it is worth investigating the agreement across all signal points considered in the original analysis, as opposed to only validating specific benchmark models.
 A comparison of the reconstruction-level and truth-level event rates before and after smearing in the signal regions SR-LM, SR-MM and SR-HM for all signal models considered in the \onelepton search is shown in~\cref{fig:smearing_signal_regions}.
 For the sake of conciseness, only the cumulative $\mct$ bins are shown in each signal region in~\cref{fig:smearing_signal_regions}.
 The agreement in the individual $\mct$ bins in each SR-LM, SR-MM and SR-HM is provided in~\cref{fig:smearing_signal_regions_1,fig:smearing_signal_regions_2,fig:smearing_signal_regions_3}.
 
The truth smearing drastically improves the agreement in event rate estimates at truth- and reconstruction-level across all \gls{sr} bins.
While, compared to reconstruction-level, the event rates are generally overestimated at truth-level before smearing, both tend to agree within statistical uncertainties after smearing. 
 
\begin{figure}
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,center},capbesidewidth=0.35\textwidth}}]{figure}[\FBwidth]
{\caption{Expected and observed exclusion contours obtained with the full likelihood using reconstruction-level inputs (orange) as well as truth-level inputs before (purple) and after (green) smearing. Uncertainties include all statistical and systematic uncertainties on the background and signal for the reconstruction-level contours, but only statistical and systematic uncertainties on the background for truth-level signal inputs.}\label{fig:simplified_likelihood_after_smearing}}
{\includegraphics[width=0.60\textwidth]{exclusion_1Lbb_truthInput_compareReco_BkgOnly_noLabel}}
\end{figure}
 
\subsection{Validation using the likelihood}

Using the nominal expected event rates at (smeared) truth-level for every signal model in the original signal grid considered in the \onelepton search, expected and observed CL$_s$ values can be computed and exclusion contours can be derived.
\Cref{fig:simplified_likelihood_after_smearing} compares the expected and observed exclusion contours obtained using the full likelihood and reconstruction-level signal inputs with those obtained using the full likelihood and truth-level signal inputs before and after truth smearing.
While all systematic uncertainties on the signal are included in the reconstruction-level contours, no signal uncertainties are considered when obtaining both the smeared and unsmeared truth-level contours. The full treatment of the systematic uncertainties on the background estimates is performed in both cases.
As expected from the previous validation steps in the signal regions, the sensitivity using unsmeared truth-level signal inputs is significantly overestimated compared to the published analysis exclusion limit using reconstruction-level inputs.
The smeared truth-level inputs, however, yield exclusion contours with an acceptable match compared to the reconstruction-level results.

In summary, the above validation process, performed at multiple selection levels of the analysis, shows that the truth-level analysis with dedicated smearing functions yields a reasonable approximation of the signal pipeline.
For signal models producing final states with kinematics close to those of the scenarios validated in the previous sections, this approach allows to determine the event rate estimates with high computational efficiency.
In large-scale reinterpretations, the smeared truth-level analysis can be used as a basis for an efficient classification of models into two categories: models that are safely excluded (or not excluded) based on truth-level analysis only, and models where (non-)exclusion is in doubt and instead the precision of the full analysis pipeline using \textsc{Recast} is required.


